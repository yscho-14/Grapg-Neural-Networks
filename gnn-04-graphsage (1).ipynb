{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=Warning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:28.687929Z","iopub.execute_input":"2026-02-18T01:47:28.688206Z","iopub.status.idle":"2026-02-18T01:47:29.827619Z","shell.execute_reply.started":"2026-02-18T01:47:28.688181Z","shell.execute_reply":"2026-02-18T01:47:29.826910Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. GraphSAGE model with Pubmed \"Big\" Dataset","metadata":{}},{"cell_type":"code","source":"# 1. library\nimport torch\n\nfind_links = f\"https://data.pyg.org/whl/torch-{torch.__version__}.html\"\n\n!pip install -q \\\n    torch-scatter \\\n    torch-sparse \\\n    torch-cluster \\\n    torch-spline-conv \\\n    torch-geometric \\\n    -f $find_links\n\n# seed set\ntorch.manual_seed(0)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nprint(\"Installation Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:29.829045Z","iopub.execute_input":"2026-02-18T01:47:29.829513Z","iopub.status.idle":"2026-02-18T01:47:40.145992Z","shell.execute_reply.started":"2026-02-18T01:47:29.829488Z","shell.execute_reply":"2026-02-18T01:47:40.145102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cora dataset, papers as nodes and their citations as edges\nfrom torch_geometric.datasets import Planetoid\n\n# Loading Cora dataset\n# Graph Data Download by Planetoid\ndataset = Planetoid(root=\"./cora_data\", name=\"Pubmed\")\ndata = dataset[0]\n\n# Print information about the dataset\nprint(f'Dataset: {dataset}')\nprint('-------------------')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of nodes: {data.x.shape[0]}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')\n\n# Print information about the graph\nprint(f'\\nGraph:')\nprint('------')\nprint(f'Training nodes: {sum(data.train_mask).item()}')\nprint(f'Evaluation nodes: {sum(data.val_mask).item()}')\nprint(f'Test nodes: {sum(data.test_mask).item()}')\nprint(f'Edges are directed: {data.is_directed()}')\nprint(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Graph has loops: {data.has_self_loops()}')\n\n# The PubMed dataset is a citation network of biomedical publications related to diabetes classification.\n# The dataset consists of 19,717 scientific papers and 44,338 citation links (edges) with TF-IDF weighted word vectors.\n# Bif dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:40.147397Z","iopub.execute_input":"2026-02-18T01:47:40.148062Z","iopub.status.idle":"2026-02-18T01:47:53.493957Z","shell.execute_reply.started":"2026-02-18T01:47:40.148033Z","shell.execute_reply":"2026-02-18T01:47:53.493161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batche neighbor sampling\n\nfrom torch_geometric.loader import NeighborLoader\nfrom torch_geometric.utils import to_networkx\n\n# Create batches with neighbor sampling\ntrain_loader = NeighborLoader(\n    data, # \n    num_neighbors=[5, 10], # Sample 5 neighbors for the 1st hop, 10 for the 2nd hop\n    batch_size=16,  # Number of seed nodes (center nodes) per batchseed nodes : 16 + 80 (5*16) + 800 (80*10)\n    input_nodes=data.train_mask, # Only use training nodes as seed nodes\n)\n\n# Inspect the created subgraphs (mini-batches)\nfor i, subgraph in enumerate(train_loader):\n    print(f'Subgraph {i}: {subgraph}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:53.495003Z","iopub.execute_input":"2026-02-18T01:47:53.495528Z","iopub.status.idle":"2026-02-18T01:47:53.513290Z","shell.execute_reply.started":"2026-02-18T01:47:53.495493Z","shell.execute_reply":"2026-02-18T01:47:53.512709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import networkx as nx\nfrom matplotlib.lines import Line2D # Tool for creating custom legends\nfrom matplotlib.colors import ListedColormap # Tool for custom color maps\n\n# 1. Define Class Names (PubMed Standard Mapping)\nclass_map = {\n    0: 'Experimental',    # lightcoral \n    1: 'Type 1 Diabetes', # lightgreen\n    2: 'Type 2 Diabetes'  # lightskyblue\n}\n\n# 2. Visualization Setup\nfig = plt.figure(figsize=(10, 8))\n\n# Define custom colors: 0->Red, 1->Green, 2->Blue\ncustom_colors = ['lightcoral', 'lightgreen', 'lightskyblue']\ncmap = ListedColormap(custom_colors)\n\n# 3. Loop to Plot Subgraphs\n# Use zip() to plot only the first 4 subgraphs\nfor idx, (subdata, pos) in enumerate(zip(train_loader, [221, 222, 223, 224])):\n    \n    # Convert PyG data to NetworkX graph\n    G = to_networkx(subdata, to_undirected=True)\n    \n    # Add subplot\n    ax = fig.add_subplot(pos)\n    ax.set_title(f'Subgraph {idx} (Batch Size: {subdata.batch_size})', fontsize=10, fontweight='bold')\n    plt.axis('off')\n    \n    # Calculate layout (Fix seed for reproducibility)\n    pos_layout = nx.spring_layout(G, seed=1, k=0.5)\n    \n    # Draw Graph\n    # Colors are automatically assigned based on node_color values (0, 1, 2) and the cmap\n    nx.draw_networkx(G,\n                     pos=pos_layout,\n                     with_labels=False,\n                     node_color=subdata.y,\n                     node_size=80,\n                     cmap=cmap,         # Use the custom colormap (Red, Green, Blue)\n                     vmin=0, vmax=2,    # Fix the range for classes 0 to 2\n                     edge_color='gray',\n                     alpha=0.8\n                     )\n\n# 4. Create a Unified Legend\n# Add a legend at the top to explain what each color represents\nlegend_elements = [\n    Line2D([0], [0], marker='o', color='w', label=class_map[0],\n           markerfacecolor=custom_colors[0], markersize=8), # Red\n    Line2D([0], [0], marker='o', color='w', label=class_map[1],\n           markerfacecolor=custom_colors[1], markersize=8), # Green\n    Line2D([0], [0], marker='o', color='w', label=class_map[2],\n           markerfacecolor=custom_colors[2], markersize=8)  # Blue\n]\n\n# Place the legend at the upper center of the figure\nfig.legend(handles=legend_elements, loc='upper center', ncol=3, fontsize=10, frameon=False)\n\nplt.tight_layout(rect=[0, 0, 1, 0.95]) # Adjust layout to make room for the legend\nplt.show()\n\n# the massive citation network is sampled into manageable subgraphs.\n# The graph shows a high degree of heterophily, where connected nodes often belong to different classes.\n# GAT is preferred over GCN as its attention mechanism can filter out irrelevant neighbors, unlike GCN's static averaging.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:53.514947Z","iopub.execute_input":"2026-02-18T01:47:53.515234Z","iopub.status.idle":"2026-02-18T01:47:54.969531Z","shell.execute_reply.started":"2026-02-18T01:47:53.515209Z","shell.execute_reply":"2026-02-18T01:47:54.968616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Implementation of GraphSAGE model architecture and mini-batch training loop\n\nimport torch\ntorch.manual_seed(1)\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv\n\n# 1. Helper function to calculate classification accuracy\ndef accuracy(pred_y, y):\n    return ((pred_y == y).sum() / len(y)).item()\n\n# 2. GraphSAGE Model Definition\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, dim_in, dim_h, dim_out):\n        super().__init__()\n        \n        # [Layer 1] First GraphSAGE Convolution Layer\n        # Input: dim_in (Number of features) -> Output: dim_h (Hidden dimension)\n        # SAGEConv aggregates information from sampled neighbors (mean, max, etc.)\n        self.sage1 = SAGEConv(dim_in, dim_h)\n        \n        # [Layer 2] Second GraphSAGE Convolution Layer\n        # Input: dim_h (Hidden dimension) -> Output: dim_out (Number of classes)\n        self.sage2 = SAGEConv(dim_h, dim_out)\n\n    def forward(self, x, edge_index):\n        # [Step 1] First Aggregation & Transformation\n        # Pass input features and connectivity to the first layer\n        h = self.sage1(x, edge_index)\n        \n        # [Step 2] Activation Function (ReLU)\n        # Apply non-linearity to the hidden features\n        h = torch.relu(h)\n        \n        # [Step 3] Dropout (Regularization)\n        # Randomly zero out 50% of the neurons to prevent overfitting during training\n        h = F.dropout(h, p=0.5, training=self.training)\n        \n        # [Step 4] Second Aggregation (Output Layer)\n        # Produce the final output scores (logits) for each class\n        h = self.sage2(h, edge_index)\n        \n        return h\n\n    # 3. Training Loop (Mini-batch Training)\n    # Training the model using mini-batches (subgraphs) for scalability\n    def fit(self, loader, epochs):\n        # Define Loss function (CrossEntropy) and Optimizer (Adam)\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n\n        self.train() # Set model to training mode\n        \n        for epoch in range(epochs+1):\n            total_loss = 0\n            acc = 0\n            val_loss = 0\n            val_acc = 0\n\n            # Iterate over batches (Subgraphs)\n            # Unlike GCN/GAT which use the full graph, GraphSAGE iterates through small parts\n            for batch in loader:\n                optimizer.zero_grad() # Clear gradients\n                \n                # Forward pass on the subgraph (mini-batch)\n                out = self(batch.x, batch.edge_index)\n                \n                # Calculate Loss only on training nodes within the batch\n                loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n                \n                total_loss += loss.item()\n                \n                # Calculate Accuracy for the batch\n                acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n                \n                # Backward pass & Weight update\n                loss.backward()\n                optimizer.step()\n\n                # Validation on the batch \n                val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n                val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n\n            # Print metrics every 20 epochs\n            if epoch % 20 == 0:\n                # Calculate average loss and accuracy across all batches\n                avg_loss = total_loss / len(loader)\n                avg_acc = acc / len(loader)\n                avg_val_loss = val_loss / len(loader)\n                avg_val_acc = val_acc / len(loader)\n                \n                print(f'Epoch {epoch:>3} | Train Loss: {avg_loss:.3f} | Train Acc: {avg_acc*100:>6.2f}% | '\n                      f'Val Loss: {avg_val_loss:.2f} | Val Acc: {avg_val_acc*100:.2f}%')\n\n    # 4. Evaluation (Inference)\n    @torch.no_grad() # Disable gradient calculation\n    def test(self, data):\n        self.eval() # Set model to evaluation mode\n        \n        # Forward pass on the full graph (or test batch)\n        out = self(data.x, data.edge_index)\n        \n        # Calculate accuracy on the test set\n        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n        return acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:54.970665Z","iopub.execute_input":"2026-02-18T01:47:54.971167Z","iopub.status.idle":"2026-02-18T01:47:54.983888Z","shell.execute_reply.started":"2026-02-18T01:47:54.971140Z","shell.execute_reply":"2026-02-18T01:47:54.983304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the Model\n\n# [Model Instantiation]\n# Initialize GraphSAGE model with dataset specifications\n# Input: 500 features (PubMed), Hidden: 64 units, Output: 3 classes\ngraphsage = GraphSAGE(dataset.num_features, 64, dataset.num_classes)\nprint(\"\\n[Model Structure]\")\nprint(graphsage)\n\n# Training\n# Train the model using the NeighborLoader (mini-batches)\nprint(\"\\n[Starting Training]\")\ngraphsage.fit(train_loader, 200)\n\n# Testing\n# Evaluate the model on the full graph using the test mask\nprint(\"\\n[Final Evaluation]\")\nacc = graphsage.test(data)\nprint(f'GraphSAGE Test Accuracy: {acc*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:54.984702Z","iopub.execute_input":"2026-02-18T01:47:54.985002Z","iopub.status.idle":"2026-02-18T01:47:58.871610Z","shell.execute_reply.started":"2026-02-18T01:47:54.984968Z","shell.execute_reply":"2026-02-18T01:47:58.870838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inductive Node Classification using GraphSAGE\n# Train a GraphSAGE model on a multi-graph dataset using neighbor sampling and evaluate on unseen graphs\n\nimport torch\nfrom sklearn.metrics import f1_score\nfrom torch_geometric.datasets import PPI\nfrom torch_geometric.data import Batch\nfrom torch_geometric.loader import DataLoader, NeighborLoader\nfrom torch_geometric.nn import GraphSAGE\n\n# 1. Device Configuration\n# Use GPU if available for faster computation.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 2. Data Loading & Preprocessing\n# Load the PPI dataset.\n# Unlike Cora (single graph), this dataset consists of multiple graphs.\n# - Train: 20 graphs\n# - Val: 2 graphs\n# - Test: 2 graphs\ntrain_dataset = PPI(root=\".\", split='train')\nval_dataset = PPI(root=\".\", split='val')\ntest_dataset = PPI(root=\".\", split='test')\n\n# Merge training graphs into a single large graph (Batch).\n# This allows 'NeighborLoader' to sample subgraphs across all training graphs simultaneously.\ntrain_data = Batch.from_data_list(train_dataset)\n\n# Initialize NeighborLoader for the training set.\n# - batch_size: Number of seed nodes per batch.\n# - num_neighbors: [20, 10] means sampling 20 neighbors at 1-hop and 10 at 2-hop.\n# - shuffle: Randomly shuffle data for stochastic gradient descent.\ntrain_loader = NeighborLoader(train_data, batch_size=2048, shuffle=True, num_neighbors=[20, 10], num_workers=2, persistent_workers=True)\n\n# Loaders for validation and testing.\n# Since validation/test graphs are relatively small, we load them graph-by-graph (batch_size=2 graphs).\nval_loader = DataLoader(val_dataset, batch_size=2)\ntest_loader = DataLoader(test_dataset, batch_size=2)\n\n# 3. Model Definition\n# Initialize GraphSAGE.\n# GraphSAGE is suitable for inductive learning (generalizing to unseen nodes/graphs).\nmodel = GraphSAGE(\n    in_channels=train_dataset.num_features,  # Input feature dimension\n    hidden_channels=512,                     # Hidden layer dimension\n    num_layers=2,                            # Number of GNN layers (hops)\n    out_channels=train_dataset.num_classes,  # Output dimension (Number of classes)\n).to(device)\n\n# 4. Loss & Optimizer\n# - Loss: BCEWithLogitsLoss (Binary Cross Entropy) is used for multi-label classification.\n# - Optimizer: Adam optimizer.\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n\n# 5. Training Function\ndef fit(loader):\n    model.train() # Set model to training mode\n\n    total_loss = 0\n    for data in loader:\n        data = data.to(device) # Move batch to device\n        optimizer.zero_grad()  # Reset gradients\n        \n        # Forward pass: Compute node embeddings and predictions\n        out = model(data.x, data.edge_index)\n        \n        # Calculate loss\n        loss = criterion(out, data.y)\n        \n        # Backward pass & Optimization\n        total_loss += loss.item() * data.num_graphs # Aggregate loss weighted by graph count\n        loss.backward()\n        optimizer.step()\n        \n    # Return average loss\n    return total_loss / len(loader.data)\n\n# 6. Evaluation Function\n@torch.no_grad() # Disable gradient calculation for inference\ndef test(loader):\n    model.eval() # Set model to evaluation mode\n\n    # Iterate through graphs in the loader (Validation/Test set)\n    # Note: 'next(iter(loader))' gets the first batch. For full evaluation, a loop is needed.\n    data = next(iter(loader)) \n    \n    # Predict on the full graph\n    out = model(data.x.to(device), data.edge_index.to(device))\n    \n    # Convert logits to binary predictions (Threshold = 0)\n    preds = (out > 0).float().cpu()\n\n    y, pred = data.y.numpy(), preds.numpy()\n    \n    # Calculate Micro F1-score (Standard metric for multi-label classification)\n    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\n\n# 7. Execution Loop\nprint(\"Starting Inductive GraphSAGE Training...\")\n\nfor epoch in range(301):\n    loss = fit(train_loader)\n    val_f1 = test(val_loader) # Evaluate on validation set\n    \n    if epoch % 50 == 0:\n        print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Val F1-score: {val_f1:.4f}')\n\n# Final evaluation on the test set (Unseen data)\nprint(f'Test F1-score: {test(test_loader):.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T01:47:58.872818Z","iopub.execute_input":"2026-02-18T01:47:58.873066Z","iopub.status.idle":"2026-02-18T02:05:28.216718Z","shell.execute_reply.started":"2026-02-18T01:47:58.873041Z","shell.execute_reply":"2026-02-18T02:05:28.215658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Inductive Learning on PPI Networks using GraphSAGE","metadata":{}},{"cell_type":"markdown","source":"### 1. Background: The \"Social Network\" of Proteins\n- Proteins do not function in isolation. They interact with one another to perform complex biological processes.\n- These interactions can be modeled as a Graph.\n- Nodes - proteins\n- Edges - physical interactions\n- Node Features - biological signatures (e.g., positional gene sets, motif gene sets)\n- Just like classifying a person's role based on their friends in a social network, we can predict a protein's function based on its interacting neighbors.\n### 2. The Challenge: Generalizing to Unseen Data\n- In many real-world biological applications, we encounter new tissues or organisms with proteins that the model has never seen before.\n- Traditional Transductive Learning: Memorizes the graph structure (e.g., standard GCN). It fails when new nodes are introduced.\n- Inductive Learning: Learns \"how to aggregate neighbor information\" rather than memorizing specific nodes. This allows the model to generalize to completely new graphs.\n### 3. Objective\nThe goal of this notebook is to build a GraphSAGE (Graph Sample and Aggregate) model to predict protein functions in unseen protein-protein interaction (PPI) networks.\n- Task - Multi-label Node Classification (predicting 121 biological functions)\n- Dataset - PPI Dataset (20 graphs for training, 2 for validation, 2 for testing)\n- Key Feature - The test graphs are completely distinct from the training graphs, testing the model's true inductive capability","metadata":{}},{"cell_type":"code","source":"!pip install rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:28.218817Z","iopub.execute_input":"2026-02-18T02:05:28.219535Z","iopub.status.idle":"2026-02-18T02:05:33.313877Z","shell.execute_reply.started":"2026-02-18T02:05:28.219479Z","shell.execute_reply":"2026-02-18T02:05:33.312958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.datasets import PPI\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import SAGEConv\nfrom sklearn.metrics import f1_score\n\n# Device Configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:33.315241Z","iopub.execute_input":"2026-02-18T02:05:33.315533Z","iopub.status.idle":"2026-02-18T02:05:33.321016Z","shell.execute_reply.started":"2026-02-18T02:05:33.315499Z","shell.execute_reply":"2026-02-18T02:05:33.320242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PPI Dataset Loading & Preprocessing\n\npath = '.'\nppi_train_dataset = PPI(path, split='train')\nppi_val_dataset = PPI(path, split='val')\nppi_test_dataset = PPI(path, split='test')\n\nprint(f\"Train Graphs: {len(ppi_train_dataset)}\")  # 20 graphs\nprint(f\"Val Graphs:   {len(ppi_val_dataset)}\")    # 2 graphs\nprint(f\"Test Graphs:  {len(ppi_test_dataset)}\")   # 2 graphs\nprint(f\"Number of features: {ppi_train_dataset.num_features}\") # 50 features per protein\nprint(f\"Number of classes:  {ppi_train_dataset.num_classes}\")  # 121 labels (multi-label classification)\n\n# 24 human tissues (graphs) - 20 for train, 2 for val, 2 for test\n# 1 graph - ave. 2,300 proteins (nodes), 3,4000~6,0000 edges","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:33.321969Z","iopub.execute_input":"2026-02-18T02:05:33.322308Z","iopub.status.idle":"2026-02-18T02:05:33.386135Z","shell.execute_reply.started":"2026-02-18T02:05:33.322282Z","shell.execute_reply":"2026-02-18T02:05:33.385579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting up DataLoaders to feed graphs into the model\n# PPI graphs are dense and large enough that we treat each graph as a batch.\n# batch_size=1 means we load 1 whole tissue graph at a time for training.\n\nppi_train_loader = DataLoader(ppi_train_dataset, batch_size=1, shuffle=True)\nppi_val_loader = DataLoader(ppi_val_dataset, batch_size=2, shuffle=False)\nppi_test_loader = DataLoader(ppi_test_dataset, batch_size=2, shuffle=False)\n\n# Extracting and inspecting the first batch from the training loader\n\nbatch = next(iter(ppi_train_loader)) # Get the first batch\n\nprint(\"the First Batch Structure\")\nprint(f\"1. Data Type: {type(batch)}\") \nprint(f\"2. Overall Structure: {batch}\")\nprint(f\"3. Number of Nodes: {batch.num_nodes}\") # 591 - proteins\nprint(f\"4. Number of Edges: {batch.num_edges}\") # 7708 - connectivity\nprint(f\"5. Node Feature Matrix (x) Shape: {batch.x.shape}\") # 591, 50 - proteins, features\nprint(f\"6. Label Matrix (y) Shape: {batch.y.shape}\") # 591, 121 - proteins, functions\nprint(f\"7. Edge Index Shape: {batch.edge_index.shape}\") # 2, 7708 - source & target nodes, connectivity\nprint(f\"8. Batch Vector : {batch.batch}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:33.387076Z","iopub.execute_input":"2026-02-18T02:05:33.387340Z","iopub.status.idle":"2026-02-18T02:05:33.405073Z","shell.execute_reply.started":"2026-02-18T02:05:33.387315Z","shell.execute_reply":"2026-02-18T02:05:33.404401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\nfrom torch_geometric.utils import to_networkx, k_hop_subgraph\n\n# the first graph from the 20 training graphs\nppi_data = ppi_train_dataset[0] \n\nprint(f\"Selected Graph Info: {ppi_data}\")\nprint(f\"- Num Nodes: {ppi_data.num_nodes}\")\nprint(f\"- Num Edges: {ppi_data.edge_index.shape[1]}\")\nprint(f\"- Node Feature Dim: {ppi_data.num_features}\")\n\n# Visualization\ntarget_node = 0  # Center node index to visualize\nnum_hops = 2     # 2-hop neighborhood\n\n# Extract the k-hop subgraph\nsubset, edge_index, mapping, edge_mask = k_hop_subgraph(\n    node_idx=target_node, \n    num_hops=num_hops, \n    edge_index=ppi_data.edge_index, \n    relabel_nodes=True\n)\n\n# Convert PyG Data to NetworkX Graph\ng = to_networkx(ppi_data)\n# Extract the subgraph using the subset of nodes found above\nsub_g = g.subgraph(subset.tolist())\n\nplt.figure(figsize=(10, 8))\npos = nx.spring_layout(sub_g, seed=42)  \n\n# Define Node Colors: Center(Red), 1-hop(Blue), 2-hop(Gray)\nnode_colors = []\nfor node in sub_g.nodes():\n    if node == target_node:\n        node_colors.append('lightcoral')    # Center (Target Node)\n    elif node in g.neighbors(target_node):\n        node_colors.append('lightgreen')   # 1-hop Neighbors\n    else:\n        node_colors.append('lightskyblue')   # 2-hop Neighbors\n\n# Draw Graph\nnx.draw_networkx_nodes(sub_g, pos, node_color=node_colors, node_size=100, alpha=0.9)\nnx.draw_networkx_edges(sub_g, pos, edge_color='lightgray', alpha=0.5)\nnx.draw_networkx_labels(sub_g, pos, font_size=8, font_color='black')\n\nplt.title(f\"Visualizing Neighborhood of Node {target_node} ({num_hops}-hop)\", fontsize=15)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:33.406084Z","iopub.execute_input":"2026-02-18T02:05:33.406468Z","iopub.status.idle":"2026-02-18T02:05:37.018446Z","shell.execute_reply.started":"2026-02-18T02:05:33.406439Z","shell.execute_reply":"2026-02-18T02:05:37.017779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Node Degree Distribution ()\n# the connectivity of proteins - Hub identification\n\nd = np.array([val for (node, val) in g.degree()])\n\n# Visualization\nplt.figure(figsize=(8, 3))\nplt.hist(d, bins=50, color='skyblue', edgecolor='black')\n\nplt.title(\"Node Degree Distribution (Connectivity)\", fontsize=12)\nplt.xlabel(\"Number of Connections (Degree)\")\nplt.ylabel(\"Count of Proteins\")\nplt.grid(axis='y', alpha=0.5)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.020908Z","iopub.execute_input":"2026-02-18T02:05:37.021203Z","iopub.status.idle":"2026-02-18T02:05:37.179994Z","shell.execute_reply.started":"2026-02-18T02:05:37.021177Z","shell.execute_reply":"2026-02-18T02:05:37.179405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# the distribution of 50 features associated with each node\n# visualize only the first 50 nodes.\n\nplt.figure(figsize=(8, 4))\n\nfeatures = ppi_data.x[:50, :].numpy()  # Slice: (50 Nodes, 50 Features)\n\n# Heatmap\nsns.heatmap(features, cmap=\"viridis\", cbar=True)\n\nplt.title(\"Feature Matrix Heatmap (First 50 Nodes)\", fontsize=12)\nplt.xlabel(\"Features (Gene Signatures, Motif sets, etc.)\")\nplt.ylabel(\"Nodes (Proteins)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.180821Z","iopub.execute_input":"2026-02-18T02:05:37.181061Z","iopub.status.idle":"2026-02-18T02:05:37.457542Z","shell.execute_reply.started":"2026-02-18T02:05:37.181035Z","shell.execute_reply":"2026-02-18T02:05:37.456976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GraphSAGE Architectureimport \n\nimport torch.nn as nn\n\nclass GraphSAGE_Advanced(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):\n        super().__init__()\n        \n        # ModuleList: A list to store layers (useful for loops)\n        self.convs = nn.ModuleList() \n        self.norms = nn.ModuleList()\n        \n        # Input Projection: Transform input features to hidden dimension first\n        self.lin = nn.Linear(in_channels, hidden_channels) \n        \n        # Stacking Layers (Hidden -> Hidden)\n        for _ in range(num_layers):\n            # SAGEConv: Aggregates neighbor information (mean aggregation)\n            self.convs.append(SAGEConv(hidden_channels, hidden_channels, aggr='mean'))\n            # LayerNorm: Stabilizes training by normalizing features\n            self.norms.append(nn.LayerNorm(hidden_channels))\n        \n        # Output Layer: Map hidden features to final output classes\n        self.output_layer = nn.Linear(hidden_channels, out_channels)\n\n    def forward(self, x, edge_index):\n        # [Step 1] Input Projection\n        # Resize input features to match hidden dimension for skip connections\n        x = self.lin(x)\n        \n        # Save current state for residual connection (Skip Connection)\n        x_prev = x \n\n        # [Step 2] Message Passing Loop (Deep GraphSAGE)\n        for conv, norm in zip(self.convs, self.norms):\n            # 1. Graph Convolution (Neighbor Aggregation)\n            x = conv(x, edge_index)\n            \n            # 2. Normalization (Stability)\n            x = norm(x)\n            \n            # 3. Activation Function (ELU)\n            # ELU (Exponential Linear Unit) is often robust for deep GNNs\n            x = F.elu(x) \n            \n            # 4. Dropout (Regularization)\n            x = F.dropout(x, p=0.2, training=self.training)\n            \n            # Add previous info to current info\n            x = x + x_prev \n            \n            # Update previous state for the next layer\n            x_prev = x\n\n        # [Step 3] Final Classification\n        x = self.output_layer(x)\n        return x\n\n# Model Instantiation\nmodel = GraphSAGE_Advanced(\n    in_channels=train_dataset.num_features, \n    hidden_channels=512, \n    out_channels=train_dataset.num_classes,\n    num_layers=4\n).to(device)\n\nprint(\"[Upgraded Model Structure]\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.458489Z","iopub.execute_input":"2026-02-18T02:05:37.459065Z","iopub.status.idle":"2026-02-18T02:05:37.504542Z","shell.execute_reply.started":"2026-02-18T02:05:37.459037Z","shell.execute_reply":"2026-02-18T02:05:37.503994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss & Optimizer\n# BCEWithLogitsLoss is standard for Multi-label Classification\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.505344Z","iopub.execute_input":"2026-02-18T02:05:37.505618Z","iopub.status.idle":"2026-02-18T02:05:37.510157Z","shell.execute_reply.started":"2026-02-18T02:05:37.505593Z","shell.execute_reply":"2026-02-18T02:05:37.509493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Function\ndef train():\n    model.train()\n    total_loss = 0\n    \n    for ppi_data in ppi_train_loader:\n        ppi_data = ppi_data.to(device)\n        optimizer.zero_grad()\n        \n        out = model(ppi_data.x, ppi_data.edge_index)\n        loss = criterion(out, ppi_data.y)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * ppi_data.num_graphs\n        \n    return total_loss / len(ppi_train_loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.511070Z","iopub.execute_input":"2026-02-18T02:05:37.511327Z","iopub.status.idle":"2026-02-18T02:05:37.522246Z","shell.execute_reply.started":"2026-02-18T02:05:37.511303Z","shell.execute_reply":"2026-02-18T02:05:37.521511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation Function (Micro F1 Score)\n@torch.no_grad()\ndef test(loader):\n    model.eval()\n    \n    ys, preds = [], []\n    \n    for ppi_data in loader:\n        ppi_data = ppi_data.to(device)\n        out = model(ppi_data.x, ppi_data.edge_index)\n        \n        # Apply Sigmoid to get probabilities\n        # Threshold 0.5: If prob > 0.5, predict as Class 1\n        pred = (torch.sigmoid(out) > 0.5).float()\n        \n        ys.append(ppi_data.y.cpu())\n        preds.append(pred.cpu())\n        \n    # Concatenate all graphs in the batch\n    y = torch.cat(ys, dim=0).numpy()\n    pred = torch.cat(preds, dim=0).numpy()\n    \n    # Calculate Micro-F1 Score\n    # Micro F1 calculates metrics globally by counting the total true positives, \n    # false negatives and false positives. Good for imbalanced classes.\n    return f1_score(y, pred, average='micro')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.523095Z","iopub.execute_input":"2026-02-18T02:05:37.523338Z","iopub.status.idle":"2026-02-18T02:05:37.533215Z","shell.execute_reply.started":"2026-02-18T02:05:37.523301Z","shell.execute_reply":"2026-02-18T02:05:37.532702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Starting GraphSAGE Training\n\ntrain_losses = []\nval_f1_scores = []\n\nepochs = 200 \n\nfor epoch in range(1, epochs + 1):\n    loss = train()\n    val_f1 = test(ppi_val_loader)\n    \n    train_losses.append(loss)\n    val_f1_scores.append(val_f1)\n    \n    if epoch % 20 == 0:\n        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Micro-F1: {val_f1:.4f}')\n\n# Final Test\ntest_f1 = test(ppi_test_loader)\nprint(f'\\nFinal Test Micro-F1 Score: {test_f1:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:05:37.533919Z","iopub.execute_input":"2026-02-18T02:05:37.534100Z","iopub.status.idle":"2026-02-18T02:07:16.676668Z","shell.execute_reply.started":"2026-02-18T02:05:37.534079Z","shell.execute_reply":"2026-02-18T02:07:16.675818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\n\nplt.figure(figsize=(8, 3))\n\n# Loss Curve\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss', color='red')\nplt.title('Training Loss (BCE)')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\n# Metric Curve (F1)\nplt.subplot(1, 2, 2)\nplt.plot(val_f1_scores, label='Validation F1', color='green')\nplt.title('Validation Micro-F1 Score')\nplt.xlabel('Epoch')\nplt.ylabel('F1 Score')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T02:07:16.677721Z","iopub.execute_input":"2026-02-18T02:07:16.677971Z","iopub.status.idle":"2026-02-18T02:07:16.904125Z","shell.execute_reply.started":"2026-02-18T02:07:16.677945Z","shell.execute_reply":"2026-02-18T02:07:16.903550Z"}},"outputs":[],"execution_count":null}]}